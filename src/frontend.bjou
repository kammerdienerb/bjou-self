# frontend.bjou

module frontend

using import "measure_time.bjou"
using import "thread.bjou"
using import "threadpool.bjou"
using import "bucket_array.bjou"
using import "io.bjou"
using import "hash_table.bjou"
using import "hash_set.bjou"
using import "pair.bjou"


using import "globals.bjou"
using import "compilation.bjou"
using import "ast.bjou"
using import "utils.bjou"
using import "parser.bjou"
using import "hash_functions.bjou"
using import "scope.bjou"
using import "ty.bjou"


type async_module_scope_info {
    idx   : i32
    nodes : astref[...] # copied, not owned
    fe    : frontend ref
}


type frontend {
    n_files            : u64
    n_lines            : u64
    n_blank_lines      : u64
    n_bytes            : u64
    pool               : (threadpool ref | none)
    all_nodes          : bucket_array$ast
    top_level_nodes    : bucket_array$astref
    include_containers : bucket_array$include_container
    files_seen         : hash_set$(string, str_hasher)
    global_scope       : globalscope
    type_nodes         : bucket_array$astref
    # @bug 9 @bug 13
    # Shouldn't have to qualify the ty module here.
    type_table         : hash_table$(string, ty::ty, str_hasher)

    node_lock              : mutex
    include_container_lock : mutex
    files_seen_lock        : mutex
    global_scope_lock      : mutex


    proc create() : frontend {
        return { frontend:
            .all_nodes              = bucket_array$ast.create(),
            .include_containers     = bucket_array$include_container.create(),
            .top_level_nodes        = bucket_array$astref.create(),
            .files_seen             = hash_set$(string, str_hasher).create(),
            .global_scope           = globalscope.create(),
            .type_nodes             = bucket_array$astref.create(),
            .type_table             = hash_table$(string, ty, str_hasher).create(),
            .node_lock              = mutex.create(),
            .include_container_lock = mutex.create(),
            .files_seen_lock        = mutex.create(),
            .global_scope_lock      = mutex.create(),
        }
    }

    proc go(this) {
        beg := measure_time_now_ms()

        if not compil.args.no_parallel {
            n_threads := compil.args.threads
            if n_threads == -1
                n_threads = thread::hw_threads() - 1

            this.pool = threadpool.create(n_threads)
        }

        # do work
        this.do_parsing()
        if compil.args.syntax_only {
            return
        }
        this.do_symbols()
        this.do_type_table()

        if pool : threadpool ref = this.pool {
            pool.stop(stopmode.GRACEFUL)
            pool.free()
        }

        end := measure_time_now_ms()

        if compil.args.stats {
            using ui
            ms := end - beg
            report_phase_time("Front-end", ms, Attr.GREEN)
        }
    }

    proc do_parsing(this) {
        parse_beg := measure_time_now_ms()
        if pool : threadpool ref = this.pool {
            this.parse_parallel(pool)
        } else {
            this.parse_serial()
        }
        parse_end := measure_time_now_ms()

        if compil.args.stats {
            using ui
            ms := parse_end - parse_beg
            phase_str := str("Parsed ")
            phase_str  = concat(phase_str, str(this.n_files), str(" files"))
            report_phase_time(phase_str.c_str(), ms, Attr.YELLOW)
            phase_str.free()
        }
    }

    proc parse_serial(this) {
        foreach ref path in compil.args.input {
            f        := open_file(path, search_paths: false)
            new_path := str(f.path)

            if stored_path : string ref = this.first_encounter_of_file(new_path) {
                p := parser.create(this, stored_path, f, kind: parserkind.IMPORT)
                p.go()
            } else {
                input_repeat_error(path)
            }

            new_path.free()
        }
    }

    proc parse_parallel(this, pool : threadpool ref) {
        foreach ref path in compil.args.input {
            f        := open_file(path, search_paths: false)
            new_path := str(f.path)


            if stored_path : string ref = this.first_encounter_of_file_locked(new_path) {
                p := new async_parser
                @p = async_parser.create(this, pool, stored_path, f, kind: parserkind.INPUT_FILE)
                pool.add_task(async_parser_wrapper, p)
            } else {
                input_repeat_error(path)
            }

            new_path.free()
        }

        pool.wait()
    }

    proc do_symbols(this) {
        symbols_beg := measure_time_now_ms()

        this.symbols_top_level()

        procs := [...astref]

        filter_scopes_from_bucket_array(this.top_level_nodes, procs)

        if pool : threadpool ref = this.pool {
            this.symbols_rest_parallel(pool, procs)
        } else {
            this.symbols_rest_serial(procs)
        }

        procs.free()

        # @incomplete
        # Once we have a type system in place and working, we need to
        # verify that all procs in overload sets have unique signatures.
        #
        # Actually, maybe this should be done on the fly also as we type-check
        # everything.

        symbols_end := measure_time_now_ms()

        if compil.args.dump_symbols {
            this.global_scope.show()
        }

        if compil.args.stats {
            using ui
            ms := symbols_end - symbols_beg
            report_phase_time("Populated symbol tables", ms, Attr.YELLOW)
        }
    }

    proc symbols_top_level(this) {
        add_nodes_to_top_level_scope(this.global_scope, this.top_level_nodes, this.type_nodes)
    }

    proc __inline__ symbols_one(this, module_idx : i32, node : ast ref) {
        if module_idx == -1 {
            recursively_build_subscopes(this.global_scope, node)
        } else {
            module_scope := this.global_scope.get_module_scope_by_idx(module_idx)
            recursively_build_subscopes(module_scope, node)
        }
    }

    proc symbols_rest_serial(this, procs : astref[...] ref) {
        foreach m_node in procs {
            # @unsafe
            # Nodes are only added to procs if they aren't 'nothing'.
            node       := unsafe_ast_ref(m_node)
            module_idx := unsafe_baseref(node).module_idx
            this.symbols_one(module_idx, node)
        }
    }

    proc symbols_module_thread(arg : void*) {
        info := arg as async_module_scope_info*

        foreach m_node in info.nodes {
            # @unsafe
            # Nodes are only added to procs if they aren't 'nothing'.
            node := unsafe_ast_ref(m_node)
            (info.fe).symbols_one(info.idx, node)
        }
        delete info
    }

    proc symbols_rest_parallel(this, pool : threadpool ref, procs : astref[...] ref) {
        max_module_idx      := this.global_scope.module_scopes.size
        procs_by_module_idx := [...astref[...]]
        procs_by_module_idx.resize(max_module_idx + 1, [...astref])

        foreach m_node in procs {
            node       := unsafe_ast_ref(m_node)
            module_idx := unsafe_baseref(node).module_idx

            procs_by_module_idx[module_idx + 1].push(m_node)
        }

        for module_idx : i32 = 0; module_idx <= max_module_idx; module_idx += 1 {
            info := new async_module_scope_info
            @info = { async_module_scope_info:
                .idx   = module_idx - 1,
                .nodes = procs_by_module_idx[module_idx],
                .fe    = this,
            }
            pool.add_task(frontend.symbols_module_thread, info as void*)
        }

        pool.wait()
    }

    proc do_type_table(this) {
        types_beg := measure_time_now_ms()

        # @performance
        # Can/should we parallelize this step somehow?

        # Make an entry in the table for every type.
        # Pre-pass to insert the ty types before filling them out.
        for it := this.type_nodes.iter(); not it.is_end(); it.next() {
            # @unsafe
            # This node must be valid because it is added to the bucket
            # array in a block that requires it to be valid.
            # See add_single_node_to_top_level_scope() and friends.
            node := unsafe_ast_ref(it.val())

            if decl : declaration = node {
                this.add_to_type_table(node)
            }
        }

        # Now go back through and complete them.
        for it := this.type_nodes.iter(); not it.is_end(); it.next() {
            # @unsafe
            # This node must be valid because it is added to the bucket
            # array in a block that requires it to be valid.
            # See add_single_node_to_top_level_scope() and friends.
            node := unsafe_ast_ref(it.val())

            this.complete_type(node)
        }

        types_end := measure_time_now_ms()

        if compil.args.stats {
            using ui
            ms := types_end - types_beg
            report_phase_time("Completed types", ms, Attr.YELLOW)
        }
    }

    proc add_to_type_table(this, node : ast ref) {
        t : ty = { tyunknown: }

        if        td  : typedef = node {
            t = { tystruct: }
        } else if ttd : templatetypedef = node {
            t = { tytemplatestruct: }
        } else if e   : enumdef = node {
            t = { tyenum: }
        }

        if decl : declaration = node {
            this.type_table.insert(decl.name, t)
        }
    }

    proc complete_type(this, node : ast ref) {
        if decl : declaration = node {
            m_pair := this.type_table.lookup(decl.name)
            if key_val : pair$(string ref, ty ref) = m_pair {
                if        ts : tystruct = key_val.second {
                    key_val.second = tystruct.create_from_node(node)
                } else if tt : tytemplatestruct = key_val.second {
                    key_val.second = tytemplatestruct.create_from_node(node)
                } else if te : tyenum = key_val.second {
                    key_val.second = tyenum.create_from_node(node)
                }
            } else {
                debug_assert(false, "type not in type table")
            }
        } else {
            debug_assert(false, "node is not a delcaration")
        }
    }

    proc total_lines(this) : u64
        return this.n_lines

    proc code_lines(this) : u64
        return this.n_lines - this.n_blank_lines

    proc __inline__ add_new_module_scope_locked(this, name : string ref) : i32 {
        this.global_scope_lock.lock()
            idx := this._add_new_module_scope(name)
        this.global_scope_lock.unlock()
        return idx
    }

    proc __inline__ add_new_module_scope(this, name : string ref) : i32 {
        return this._add_new_module_scope(name)
    }

    proc _add_new_module_scope(this, name : string ref) : i32 {
        return this.global_scope.get_or_add_module_scope_idx(name)
    }


    proc __inline__ get_next_include_container_locked(this) : include_container ref {
        this.include_container_lock.lock()
            r := this._get_next_include_container()
        this.include_container_lock.unlock()
        return r
    }

    proc __inline__ get_next_include_container(this) : include_container ref {
        return this._get_next_include_container()
    }

    proc _get_next_include_container(this) : include_container ref {
        ic := { include_container: .nodes = bucket_array$astref.create() }
        return this.include_containers.push(ic)
    }


    proc __inline__ first_encounter_of_file_locked(this, path : string ref) : (string ref | none) {
        this.files_seen_lock.lock()
            r := this._first_encounter_of_file(path)
        this.files_seen_lock.unlock()
        return r
    }

    proc __inline__ first_encounter_of_file(this, path : string ref) : (string ref | none) {
        return this._first_encounter_of_file(path)
    }

    proc _first_encounter_of_file(this, path : string ref) : (string ref | none) {
        search := this.files_seen.lookup(path)

        if search.is_none() {
            return this.files_seen.insert(path.copy())
        }

        return nothing
    }
}


proc filter_scopes_from_bucket_array(nodes : bucket_array$astref ref, procs : astref[...] ref) {
    for it := nodes.iter(); not it.is_end(); it.next() {
        if node : ast ref = it.val() {
            if unsafe_baseref(node).get_flag(ast_flag.OPENS_SCOPE) {
                procs.push(node)
            } else if i : includestmt = node    {
                if ic : include_container ref = i.container {
                    filter_scopes_from_bucket_array(ic.nodes, procs)
                }
            }
        } else debug_assert(false, "bad node in filter_scopes_from_bucket_array")
    }
}

proc __inline__ open_file(path : string ref, search_paths : bool) : file {
    return open_file(path, search_paths, nothing)
}
proc open_file(path : string ref, search_paths : bool, from_cxt : (context ref | none)) : file {
    f : (file | none) = nothing

    if search_paths { f = search_for_file_and_open(path)     }
    else            { f = file.open(path.c_str(), file.READ) }

    if f' : file = f {
        if not f'.good() {
            open_file_error(path, from_cxt)
        }
        return f'
    }

    open_file_error(path, from_cxt)

    return { file: }
}

proc search_for_file_and_open(path : string ref) : (file | none) {
    foreach ref prefix in compil.search_paths {
        path' := prefix.copy()
        path'.append(path)
        f     := io::file.open(path'.c_str(), file.READ)
        path'.free()

        if f.good() { return f }
    }

    return nothing
}

proc open_file_error(path : string ref, from_cxt : (context ref | none)) {
    err_str := str("Unable to read file '")
    err_str.append(path)
    err_str.append("'.")

    eb := ui::error_builder.create()
        .with_message(err_str.c_str())

    if cxt : context ref = from_cxt {
        eb = eb.add_location(cxt)
    }

    eb.report()
}

proc input_repeat_error(path : string ref) {
    err_str := str("File '")
    err_str.append(path)
    err_str.append("' is repeated in the input file list.")

    ui::error(err_str.c_str())
}

proc multiple_include_error(path : string ref, intro_cxt : context ref) {
    err_str := str("File '")
    err_str.append(path)
    err_str.append("' has already been encountered and may not be included again.")

    ui::error_builder.create()
        .with_message(err_str.c_str())
        .add_location(intro_cxt)
        .report()
}
